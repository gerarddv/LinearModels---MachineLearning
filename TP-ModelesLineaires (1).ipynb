{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> TP2: Modèles linéaires<br>\n",
    "    Perceptron, Adaline, Regression Logistique et SVM</center></h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons nous intéresser à l'implémentation des modèles du perceptron ('p'), de l'Adline ('a') , de la regression logistique ('r') et des SVM ('s') dont les pseudo-codes puvent être résumés comme suit:\n",
    "\n",
    "input: Train, eta, m, MaxEp, modele\n",
    "\n",
    "init : w\n",
    "\n",
    "epoque=0\n",
    "\n",
    "while epoque<=MaxEp\n",
    "\n",
    "    err=0\n",
    "\n",
    "    for i in 1:m\n",
    "\n",
    "        choisir un exemple (x,y) de Train de façon aléatoire\n",
    "\n",
    "        h <- w*x\n",
    "\n",
    "        if((modele = 'a') or (modele = 'r'))\n",
    "\n",
    "            w <- w - eta*grad(L(w))\n",
    "\n",
    "        elif((modele = 'p') and (y*h <= 0))\n",
    "\n",
    "            w <- w - eta*grad(L(w))\n",
    "\n",
    "        elif((modele = 's') and (1-y*h >= 0))\n",
    "\n",
    "            w <- w - eta*grad(L(w))\n",
    "\n",
    "     epoque <- epoque+1\n",
    "\n",
    "output: w\n",
    "\n",
    "\n",
    "Pour une fonction de prédiction $h_\\mathbf{w}(\\mathbf{x})=w_0+\\langle \\mathbf{w},\\mathbf{x}\\rangle$; $\\mathcal{L}(\\mathbf{w})$ est la fonction de coût qui dans le cas de \n",
    "- Perceptron: $\\mathcal{L}(\\mathbf{w})=(-y*h_\\mathbf{w}(\\mathbf{x}))$,\n",
    "- Adaline: $\\mathcal{L}(\\mathbf{w})=(y-h_\\mathbf{w}(\\mathbf{x}))^2$,\n",
    "- Régression logistique: $\\mathcal{L}(\\mathbf{w})=\\log(1+e^{-yh_\\mathbf{w}(\\mathbf{x})})$,\n",
    "- SVM: $\\mathcal{L}(\\mathbf{w})=\\max(0,1-y*h_\\mathbf{w}(\\mathbf{x}))$\n",
    "\n",
    "$grad(\\mathcal{L}(\\mathbf{w}))$ est le gradient de la fonction $\\mathcal{L}(\\mathbf{w})$ dont on a vu les expressions pour ces modèles en TD.\n",
    "\n",
    "\n",
    "<font color='red'><b>Question 1:</b></font> Créer une liste de 4 éléments correspondant à l'exemple ET logique; chaque élément de la liste est une liste dont la dernière caractéristique est la classe de l'exemple et les premières caractéristiques leurs coordonnées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#ET est notre exemple\n",
    "#ici le -1 c'est le 0 si on avait raisonné en binaire\n",
    "ET=np.array([[0, 0, -1], [0, 1, -1], [1, 0, -1], [1, 1, 1]])\n",
    "#ET=[[-1, -1, -1], [-1, 1, -1], [1, -1, -1], [1, 1, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0 -1]\n",
      " [ 0  1 -1]\n",
      " [ 1  0 -1]\n",
      " [ 1  1  1]]\n"
     ]
    }
   ],
   "source": [
    "print(ET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'><b>Question 2:</b></font> Coder le programme du Perceptron (modele = 'p').\n",
    "\n",
    "Indication: Vous pouvez écrire une fonction qui calcule le produit scalaire entre un exemple $\\mathbf{x}=(x_1,\\ldots,x_d)$ et le vecteur poids $\\mathbf{w}=(w_0,w_1,\\ldots,w_d)$:\n",
    "$\\langle \\mathbf{w},\\mathbf{x} \\rangle=w_0+\\sum_{j=1}^d w_j x_j$; et une autre qui pour une matrice de données qui n'ont pas servi à apprendre le vecteur poids $\\mathbf{w}$ (i.e. la base Test), calcule le taux d'erreur sur cette base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''w=(3,) et x=(4,2) Le produit scalaire est possible car x est un vecteur ligne et w est un vecteur colonne.\n",
    "w[1:]done (2,) et x[:,1:]done (4,2) donc le nombre de ligne de colonne de x[:,1:] est le même que celui de w[1:]\n",
    "\n",
    "'''\n",
    "def h(x,p):\n",
    "    #Je veux avoir tous w sauf la 1er case(w est un array numpy et non une liste)\n",
    "    #v2:w[0]+sum(Xj*Wj for (Xj,Wj) in zip(x,w[1:])) \n",
    "    \n",
    "    return p[0]+x @ p[1:]\n",
    "'''Pour calculer le taux d'erreur, on va diviser le nombre de fois où le produit scalaire est négatif par la taille du\n",
    " de l'exemple x.En gros on fait la moyenne des exemples mal classés par rapport a la vrai sortie de l'exemple\n",
    "'''\n",
    "def taux_erreur(p):\n",
    "    \n",
    "    nb_neg=0\n",
    "    x=ET[:,:-1]\n",
    "    y=ET[:,-1]\n",
    "    for X,Y in zip(x,y):\n",
    "        #si l'exemple est mal classé\n",
    "        if Y*h(X,p)<=0:\n",
    "            nb_neg+=1\n",
    "    \n",
    "    return  nb_neg/x.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'><b>Question 3:</b></font> Appliquer le programme du perceptron sur la base du ET logique, calculer le taux d'erreur du modèle sur cette base.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le taux d'erreur sur la base du perceptron est: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "\"Train=base de données d'entraînement qui contient des exemples pour apprendre un modèle.\n",
    "\n",
    "Epoque=nombre de cycles d'apprentissage effectués.\n",
    "\n",
    "MaxEp=nombre maximum d'itérations pour l'algorithme, ce qui signifie que l'apprentissage s'arrêtera si le nombre d'itérations atteint MaxEp.\n",
    "\n",
    "m=nombre d'exemples dans Train.\n",
    "'''\n",
    "\n",
    "# Definition du pas d'apprentissage\n",
    "eta=0.1\n",
    "# Nombre d'itérations (époques)\n",
    "MaxEp = 500\n",
    "#le nombre de colonnes de w doit etre  égale au nombre de lignes de ET sinon produit matriciel impossible    \n",
    "\n",
    "\n",
    "w_p=np.zeros(ET.shape[1]) \n",
    "\n",
    "def perceptron(Train, eta, m, MaxEp,p):\n",
    "    \n",
    "    for epoque in range(MaxEp):\n",
    "        #pour chaque exemple dans Train\n",
    "        for ligne in range(m):\n",
    "        #y est la sortie associé à l'exemple x[ligne].Elle se trouve sur la dernière colonne de x qui est la classe de l'exemple\n",
    "            x=Train[ligne,:-1]\n",
    "            y=Train[ligne,-1]\n",
    "            if y*h(x,p)<=0:\n",
    "                p[0]=p[0]+eta*y\n",
    "                p[1:]=p[1:]+eta*y*x\n",
    "  \n",
    "        \n",
    "perceptron(ET,eta,ET.shape[0],MaxEp,w_p)\n",
    "loss_p=taux_erreur(w_p)\n",
    "print(\"Le taux d'erreur sur la base du perceptron est:\",loss_p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'><b>Question 4:</b></font> Coder les modèles Adaline ('a'), Régression Logistique ('r') et SVM ('s') et reporter leur erreur sur le problème jouet de ET logique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le taux d'erreur sur la base de l'adaline est: 0.0\n",
      "Le taux d'erreur sur la base de la regression logistique est: 0.0\n",
      "Le taux d'erreur sur la base de la SVM est: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "w_a=np.zeros(ET.shape[1]) \n",
    "w_r=np.zeros(ET.shape[1]) \n",
    "w_s=np.zeros(ET.shape[1]) \n",
    "\n",
    "def adaline(Train, eta, m, MaxEp,w_a):\n",
    "    for epoque in range(MaxEp):\n",
    "   \n",
    "        for ligne in range(m):\n",
    "            y=Train[ligne,-1]\n",
    "            x=Train[ligne,:-1]\n",
    "        \n",
    "            w_a[0]=w_a[0]+eta*(y-h(x,w_a))\n",
    "            w_a[1:]=w_a[1:]+eta*(y-h(x,w_a))*x\n",
    "    \n",
    "\n",
    "def regression_logistique(Train, eta, m, MaxEp,w_r):\n",
    "    for epoque in range(MaxEp):\n",
    "   \n",
    "        for ligne in range(m):\n",
    "            y=Train[ligne,-1]\n",
    "            x=Train[ligne,:-1]\n",
    "            L=(1-(1/(1+np.exp(-y*h(x,w_r)))))\n",
    "            w_r[0]=w_r[0]+eta*y*L\n",
    "            w_r[1:]=w_r[1:]+eta*L*x\n",
    " \n",
    "\n",
    "def SVM(Train, eta, m,MaxEp,w_s):\n",
    "    for epoque in range(MaxEp):\n",
    "   \n",
    "        for ligne in range(m):\n",
    "            y=Train[ligne,-1]\n",
    "            x=Train[ligne,:-1]\n",
    "            if y*h(x,w_s)<=1:\n",
    "                w_s[0]=w_s[0]+eta*y\n",
    "                w_s[1:]=w_s[1:]+eta*y*x\n",
    "           \n",
    "\n",
    "adaline(ET,eta,ET.shape[0],MaxEp,w_a)\n",
    "loss_a= taux_erreur(w_a)\n",
    "print(\"Le taux d'erreur sur la base de l'adaline est:\",loss_a)\n",
    "\n",
    "regression_logistique(ET,eta,ET.shape[0],MaxEp,w_r)\n",
    "loss_r= taux_erreur(w_r)\n",
    "print(\"Le taux d'erreur sur la base de la regression logistique est:\",loss_r)\n",
    "\n",
    "SVM(ET,eta,ET.shape[0],MaxEp,w_s)\n",
    "loss_s= taux_erreur(w_s)\n",
    "print(\"Le taux d'erreur sur la base de la SVM est:\",loss_s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'><b>Question 5:</b></font> Nous allons maintenant nous intéresser au comportement de ces modèles sur la base sonar de la collection UCI (http://archive.ics.uci.edu/ml/index.php). Cette base contient 208 exemples en dimension 60 séparés par `,` et la dernière élément correspond à la classe de l'exemple.\n",
    "\n",
    "    1. Télécharger la collection avec la fonction read_table de la librairie pandas (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_table.html). Les options nécessaires sont `sep=','` et `header=None`  \n",
    "    2. Créer une liste de listes correspondant à la collection; pour cela initialiser la première liste et en parcourant chaque ligne de la matrice de données; créer une liste associée en remplaçant le dernier élément par `-1` ou `+1` et insérer la dans la première liste. \n",
    "    Indication: Utiliser la fonction `loc`. \n",
    "    3. Scinder la liste en deux listes `x_train` (75%) and `x_test` (25%) en la mélangeant aléatoirement au préalable. \n",
    "    Indication: Utiliser les fonctions `shuffle` de la librairie `random` et `train_test_split` de la librairie `sklearn.model_selection`  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import sklearn.model_selection as sk\n",
    "from random import shuffle\n",
    "from sklearn.model_selection  import train_test_split\n",
    "\n",
    "data = pd.read_table('http://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data', sep=',', header=None)\n",
    "\n",
    "#Initialisation de la liste principale\n",
    "collection = []\n",
    "#On parcourt les lignes du dataframe\n",
    "for i in range(data.shape[0]):\n",
    "    ligne_collection=data.iloc[i].tolist()\n",
    "    #data.iloc[:,-1] = data.iloc[:,-1].replace('R',1).replace('M',-1) ou data[60] = np.where(data[60]=='R', 1, -1)\n",
    "    if ligne_collection[-1]=='R':\n",
    "       ligne_collection[-1]=1\n",
    "    else:\n",
    "        ligne_collection[-1]=-1\n",
    "    collection.append(ligne_collection)\n",
    "\n",
    "#Mélanger au préalable la colletion\n",
    "shuffle(collection)\n",
    "\n",
    "collection=np.array(collection)\n",
    "X=collection[:,:-1]\n",
    "Y=collection[:,-1]\n",
    "\n",
    "'''\n",
    "On va diviser la collection en un ensemble d'entraînement et un ensemble de test, pour pouvoir évaluer la\n",
    "performance du modèle sur des données qu'il n'a jamais vues pendant l'entraînement(qui n'a pas servi a apprendre le modéle)\n",
    "'''\n",
    "#On divise la collection en 2 parties:une pour l'entrainement(75%) et une pour le test(25%)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'><b>Question 6:</b></font> Appliquer ces modèles sur cette base en prenant comme $MaxEp=500$, le pas d'apprentissage $\\eta=0.1$ et en choisissant les bases Train et Test de façon aléatoire; Reporter l'erreur moyenne de ces modèles obtenues sur les 20 bases Test? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9607/4265622688.py:22: RuntimeWarning: overflow encountered in exp\n",
      "  L=(1-(1/(1+np.exp(-y*h(x,w_r)))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur moyenne du modèle Perceptron: 0.004807692307692308\n",
      "Erreur moyenne du modèle Adaline: 0.006730769230769232\n",
      "Erreur moyenne du modèle Regression Logistique: 0.01346153846153846\n",
      "Erreur moyenne du modèle SVM: 0.0057692307692307696\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def TAUX_ERREUR(p):\n",
    "    nb_mal_classes = 0\n",
    "    X_test=Test[:,:-1]\n",
    "    Y_test=Test[:,-1]\n",
    "    for x, y in zip(X_test, Y_test):\n",
    "            nb_mal_classes =((h(x,p)*y)<=0).sum()\n",
    "    return nb_mal_classes / len(X_test)\n",
    "\n",
    "erreur_p =[]\n",
    "erreur_a =[]\n",
    "erreur_r =[]\n",
    "erreur_s =[]\n",
    "nb_base_tests=20\n",
    "\n",
    "#A chaque itéaration,on aura des bases d'entrainement et de test différentes\n",
    "for i in range(nb_base_tests):\n",
    "    #Générer une base d'entrainement et une base de test de façon aléatoire grace a train_test_split\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25)\n",
    "\n",
    "    Test= np.column_stack((X_test, Y_test))\n",
    "    Train= np.column_stack((X_train, Y_train))\n",
    "\n",
    "    w_p=np.zeros(Train.shape[1])\n",
    "    w_a=np.zeros(Train.shape[1])\n",
    "    w_r=np.zeros(Train.shape[1])\n",
    "    w_s=np.zeros(Train.shape[1])\n",
    "\n",
    "    # On entraine les modéles \n",
    "    perceptron(Train, eta, Train.shape[0], MaxEp,w_p)\n",
    "    adaline(Train, eta, Train.shape[0], MaxEp,w_a)\n",
    "    regression_logistique(Train, eta, Train.shape[0], MaxEp,w_r)\n",
    "    SVM(Train, eta, Train.shape[0], MaxEp,w_s)\n",
    "\n",
    "    \n",
    "    # Calculer l'erreur moyenne sur les données de test\n",
    "    erreur_p.append(TAUX_ERREUR(w_p))\n",
    "    erreur_a.append(TAUX_ERREUR(w_a))\n",
    "    erreur_r.append(TAUX_ERREUR(w_r))\n",
    "    erreur_s.append(TAUX_ERREUR(w_s))\n",
    "    \n",
    "\n",
    "erreur_moyenne_p =np.mean(erreur_p)\n",
    "erreur_moyenne_a =  np.mean(erreur_a)\n",
    "erreur_moyenne_r = np.mean(erreur_r)\n",
    "erreur_moyenne_s =np.mean(erreur_s)\n",
    "\n",
    "print(\"Erreur moyenne du modèle Perceptron:\", erreur_moyenne_p)\n",
    "print(\"Erreur moyenne du modèle Adaline:\", erreur_moyenne_a)\n",
    "print(\"Erreur moyenne du modèle Regression Logistique:\", erreur_moyenne_r)\n",
    "print(\"Erreur moyenne du modèle SVM:\", erreur_moyenne_s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  | Collection | Adaline            | Perceptron          | Régression Logistique| SVM                 | \n",
    "  |------------|--------------------|---------------------|----------------------|---------------------|\n",
    "  |   SONAR    |0.006730769230769232|0.0028846153846153848|0.009615384615384614  |0.0028846153846153848|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons étudier l'impact de la nomralisation sur les prédictions. Pour cela nous considérons deux stratégies de normalisation communément utilisées dans la littérature:\n",
    "* Stratégie <i>max</i>: consiste à normaliser chaque caractéristique du vecteur réprésentatif d'une observation par la valeur maximale de cette caractéristiques\n",
    "* Stratégie <i>norme</i>: consiste à normaliser chaque caractéristique du vecteur réprésentatif d'une observation par la norme de ce vecteur.\n",
    "\n",
    "Nous considérons ces trois autres collections de la base UCI:\n",
    "\n",
    "        * https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29\n",
    "        * https://archive.ics.uci.edu/ml/datasets/spambase\n",
    "        * https://archive.ics.uci.edu/ml/datasets/ionosphere\n",
    "\n",
    "<font color='red'><b>Question 7:</b></font> Ecrire une fonction qui prend en entrée la collection des données et qui retourne la collections normalisée suivant les stratégies <i>max</i> et <i>norme</i>. \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "UFuncTypeError",
     "evalue": "ufunc 'maximum' did not contain a loop with signature matching types (dtype('<U21'), dtype('<U21')) -> None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 57\u001b[0m\n\u001b[1;32m     53\u001b[0m         collection_norme[i,:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m collection_norme[i,:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m/\u001b[39m norm\n\u001b[1;32m     55\u001b[0m     \u001b[39mreturn\u001b[39;00m collection_max, collection_norme\n\u001b[0;32m---> 57\u001b[0m collection_cancer_max, collection_cancer_norme \u001b[39m=\u001b[39m normalisation(collection_cancer)\n\u001b[1;32m     58\u001b[0m collection_spam_max, collection_spam_norme \u001b[39m=\u001b[39m normalisation(collection_spam)\n\u001b[1;32m     59\u001b[0m collection_ionosphere_max, collection_ionosphere_norme \u001b[39m=\u001b[39m normalisation(collection_ionosphere)\n",
      "Cell \u001b[0;32mIn[10], line 45\u001b[0m, in \u001b[0;36mnormalisation\u001b[0;34m(collection)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39m#collection.shape[1]-1 car on ne veut pas normaliser la dernière colonne qui est la classe\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(collection\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m     44\u001b[0m     \u001b[39m#Pour chaque colonne,on calcule le max\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     \u001b[39mmax\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mmax(collection[:,i])\n\u001b[1;32m     46\u001b[0m     collection_max[:,i] \u001b[39m=\u001b[39m collection_max[:,i] \u001b[39m/\u001b[39m \u001b[39mmax\u001b[39m\n\u001b[1;32m     48\u001b[0m \u001b[39m# Stratégie norme\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mamax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:2820\u001b[0m, in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2703\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_amax_dispatcher)\n\u001b[1;32m   2704\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mamax\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue, initial\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue,\n\u001b[1;32m   2705\u001b[0m          where\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue):\n\u001b[1;32m   2706\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2707\u001b[0m \u001b[39m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[1;32m   2708\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2818\u001b[0m \u001b[39m    5\u001b[39;00m\n\u001b[1;32m   2819\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2820\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapreduction(a, np\u001b[39m.\u001b[39;49mmaximum, \u001b[39m'\u001b[39;49m\u001b[39mmax\u001b[39;49m\u001b[39m'\u001b[39;49m, axis, \u001b[39mNone\u001b[39;49;00m, out,\n\u001b[1;32m   2821\u001b[0m                           keepdims\u001b[39m=\u001b[39;49mkeepdims, initial\u001b[39m=\u001b[39;49minitial, where\u001b[39m=\u001b[39;49mwhere)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m             \u001b[39mreturn\u001b[39;00m reduction(axis\u001b[39m=\u001b[39maxis, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[39mreturn\u001b[39;00m ufunc\u001b[39m.\u001b[39;49mreduce(obj, axis, dtype, out, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpasskwargs)\n",
      "\u001b[0;31mUFuncTypeError\u001b[0m: ufunc 'maximum' did not contain a loop with signature matching types (dtype('<U21'), dtype('<U21')) -> None"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Un vecteur=[....]donc les observations sont les lignes de la collection et les caractéristiques sont les colonnes\n",
    "Donc le max sera le max de la colonne\n",
    "Pour chaque observation,on a plusieurs caractéristiques et la dernière caractéristique est la classe\n",
    "'''\n",
    "'''\n",
    "On évitera d'écrire data.iloc[:,-1]=data.iloc[:,-1].replace({unique_values[0]: 1, unique_values[1]: -1}) ou data.loc[:,data.columns[-1]]=data.loc[:,data.columns[-1]].replace({unique_values[0]: 1, unique_values[1]: -1}) \n",
    "pour ne pas avoir l'avertissement de dépréciation, qui indique que dans une future version de pandas, l'utilisation de la méthode iloc,loc\n",
    "avec une affectation directe ne sera plus recommandée.\n",
    "'''\n",
    "\n",
    "def dataFrame_to_collection(data):\n",
    "    collections = []\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "        unique_values =data.iloc[:,-1].unique()#ici c'est que 2 valeurs unique\n",
    "      \n",
    "        data[data.columns[-1]] = data[data.columns[-1]].replace({unique_values[0]: 1, unique_values[1]: -1})\n",
    "       \n",
    "        ligne_collections=data.iloc[i].tolist()\n",
    "       \n",
    "        collections.append(ligne_collections)\n",
    "\n",
    "    return np.array(collections)\n",
    "\n",
    "\n",
    "data_cancer=pd.read_table('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data', sep=',', header=None)\n",
    "data_spam=pd.read_table('https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data', sep=',', header=None)\n",
    "data_ionosphere=pd.read_table('https://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data', sep=',', header=None)\n",
    "\n",
    "\n",
    "collection_cancer=dataFrame_to_collection(data_cancer)\n",
    "\n",
    "\n",
    "collection_spam=dataFrame_to_collection(data_spam)\n",
    "\n",
    "collection_ionosphere=dataFrame_to_collection(data_ionosphere)\n",
    "\n",
    "def normalisation(collection):\n",
    "    # Stratégie max\n",
    "    collection_max = collection.copy()\n",
    "    #collection.shape[1]-1 car on ne veut pas normaliser la dernière colonne qui est la classe\n",
    "    for i in range(collection.shape[1]-1):\n",
    "        #Pour chaque colonne,on calcule le max\n",
    "        max= np.max(collection[:,i])\n",
    "        collection_max[:,i] = collection_max[:,i] / max\n",
    "        \n",
    "    # Stratégie norme\n",
    "    collection_norme = collection.copy()\n",
    "    for i in range(collection.shape[0]):\n",
    "        vecteur=collection[i,:-1]\n",
    "        norm = np.linalg.norm(vecteur)\n",
    "        collection_norme[i,:-1] = collection_norme[i,:-1] / norm\n",
    "        \n",
    "    return collection_max, collection_norme\n",
    "\n",
    "collection_cancer_max, collection_cancer_norme = normalisation(collection_cancer)\n",
    "collection_spam_max, collection_spam_norme = normalisation(collection_spam)\n",
    "collection_ionosphere_max, collection_ionosphere_norme = normalisation(collection_ionosphere)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'><b>Question 8:</b></font> Compléter les tableaux comparatifs suivants en repertant les erreurs moyennes sur 20 lancements des modèles de Perceptron, de l'Adaline, de la Régression Logistique et des SVM pour les trois cas:\n",
    "\n",
    " '*' Les vecteurs ne sont pas normalisés\n",
    "     \n",
    "  | Collection | Adaline     | Perceptron  | Régression Logistique | SVM |\n",
    "  |------------|-------------|-------------|-----------------------|-----|\n",
    "  |   BREAST   |             |             |                       |     |\n",
    "  |   IONO     |             |             |                       |     |\n",
    "  |   SONAR    |             |             |                       |     |\n",
    "  |   SPAM     |             |             |                       |     |\n",
    "\n",
    " \n",
    " $^n$ Normalisation suivant la stratégie <i>norme</i>\n",
    "     \n",
    "  | Collection | Adaline     | Perceptron  | Régression Logistique | SVM |\n",
    "  |------------|-------------|-------------|-----------------------|-----|\n",
    "  |   BREAST   |             |             |                       |     |\n",
    "  |   IONO     |             |             |                       |     |\n",
    "  |   SONAR    |             |             |                       |     |\n",
    "  |   SPAM     |             |             |                       |     |\n",
    "\n",
    "  \n",
    " $^m$ Normalisation suivant la stratégie <i>max</i>\n",
    "    \n",
    "  | Collection | Adaline     | Perceptron  | Régression Logistique | SVM |\n",
    "  |------------|-------------|-------------|-----------------------|-----|\n",
    "  |   BREAST   |             |             |                       |     |\n",
    "  |   IONO     |             |             |                       |     |\n",
    "  |   SONAR    |             |             |                       |     |\n",
    "  |   SPAM     |             |             |                       |     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron(Train, eta, Train.shape[0], MaxEp,w_p)\n",
    "adaline(Train, eta, Train.shape[0], MaxEp,w_a)\n",
    "regression_logistique(Train, eta, Train.shape[0], MaxEp,w_r)\n",
    "SVM(Train, eta, Train.shape[0], MaxEp,w_s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
